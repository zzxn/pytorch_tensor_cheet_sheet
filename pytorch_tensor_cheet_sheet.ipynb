{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改变形状\n",
    "\n",
    "* `torch.reshape(input, shape)`：改变形状，可以有一个维度是-1，会 **尽可能** 返回原始数据的一个视图（view）而不进行数据拷贝\n",
    "* `torch.Tensor.view(*shape)`：改变形状，可以有一个维度是-1，会返回原始数据的一个视图（即与原tensor共享数据），若不能则报错\n",
    "* `torch.squeeze(input, dim=None)`：把 **所有** 大小为1的维度移除，当`dim`指定时，只移除该维度（大小必须为1）\n",
    "* `torch.unsqueeze(input, dim)`：在`dim`处插入一个大小为1的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交换维度（转置）\n",
    "\n",
    "* `torch.Tensor.transpose(dim0, dim1)`：交换两个维度，返回原始tensor的视图\n",
    "* `torch.Tensor.permute(*dims)`：按照`*dims`指定的顺序对维度进行排序，返回原始tensor的视图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复制维度\n",
    "\n",
    "* `torch.Tensor.expand(*sizes)`：将tensor的形状广播为`*sizes`，返回一个视图\n",
    "    * 不想改动的维度可设置为-1\n",
    "    * 可在开头插入新维度，但不能删除维度，且插入的新维度不能设为-1\n",
    "    * 扩展后的tensor中或许有多个元素共享存储空间，这可能导致错误，所以如果要对它写，先进行复制\n",
    "* `torch.Tensor.repeat(*sizes)`：行为与`expand`相同，但是会拷贝数据\n",
    "* `torch.repeat_interleave(input, repeats, dim=None) `：不常用，自行查阅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复制\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼接\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拆分\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "\n",
    "PyTorch和Numpy中的许多针对tensor或者ndarray的操作都要求两者的形状相同，但是有的时候我们想有种机制**自动地去复制某些维度**，以在不通过复制数据的情况下让tensor的形状相匹配，这种机制就叫做**广播**。\n",
    "\n",
    "许多PyTorch操作支持Numpy的[广播语义](https://numpy.org/doc/stable/user/basics.broadcasting.html)。\n",
    "\n",
    "广播就是对于两个形状不同的tensor，在概念上把互相不匹配的维度自动地进行扩展，使得两个tensor的形状相同，避免不必要的复制。\n",
    "\n",
    "上面的定义暗示了不是任何两个tensor都可以进行广播，它们的形状必须是兼容的，或者说**可广播的**（boradcastable）。\n",
    "\n",
    "当满足下面条件时，两个tensor是可广播的：\n",
    "\n",
    "* 每个tensor都至少有1个维度（及形状都不能是`torch.Size([])`）\n",
    "* 从最后一个维度开始往前进行比较，两个维度必须是相等的，或者其中一个为1，或者其中一个不存在\n",
    "\n",
    "可广播的两个tensor，在从后往前比较各维度的过程中有三种情况：\n",
    "\n",
    "* 如果两个维度相等，则什么都不做\n",
    "* 如果其中一个为1，另一个为k，则把为1的那个tensor的这个维度复制k次，使得两个维度相等\n",
    "* 如果其中一个不存在，另一个为k，则把不存在的那个tensor在这个位置增加一个维度，把该维度的数据复制k次，使得两个维度相等\n",
    "\n",
    "注意，维度不存在情况只能出现在维度较少的tensor的开头。\n",
    "\n",
    "下面是一些例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(5, 3, 4, 1)\n",
    "b = torch.randn(   3, 1, 1)\n",
    "(a + b).shape  # of shape (5, 3, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5, 3, 1, 2)\n",
    "b = torch.randn(5, 1, 4, 2)\n",
    "(a + b).shape  # of shape (5, 3, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, 3, 1, 2)\n",
    "b = torch.randn(5, 2, 4, 2)\n",
    "# (a + b).shape  # RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 注意广播后的结果有可能不同于原来的任何一个矩阵，而in-place操作（例如`Tensor.add_`等末尾带`_`的函数，或者它的等价形式`a += b`）不允许改变原有tensor的形状，这样的话一些in-place的操作有可能触发`RuntimeError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3, 1, 4)\n",
    "b = torch.randn(3, 5, 1)\n",
    "# a += b  # RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在老版本的PyTorch中，一些逐点（pointwise）操作只要求两个tensor的元素个数一样，然后把两个tensor视作一维后逐点操作，而在引入广播语义后，这些操作会对两个tensor做广播，这可能导致一些向后兼容性问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 1)\n",
    "b = torch.randn(   4)\n",
    "(a + b).shape  # result is of shape(4, 1) in prior versions, of shape(4, 4) in the current version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加减乘除\n",
    "\n",
    "这些操作都是二元操作，并且一般可以通过指定一个`out`参数来指示结果存放的tensor。\n",
    "\n",
    "### 加法\n",
    "\n",
    "* `a + b`相当于`torch.add(a, b)`\n",
    "* `torch.add`：逐点相加，支持广播\n",
    "* `torch.sum`：默认返回所有元素之和\n",
    "    * 若指定`dim`参数，则沿着该维度求和\n",
    "    * `dim`可以是一个列表\n",
    "    * 求和后`dim`指示的维度消失，如果不想让它消失，则指定`keepdim=True`。\n",
    "\n",
    "### 减法\n",
    "\n",
    "* `a - b`相当于`torch.sub(a, b)`\n",
    "* `torch.sub`：逐点减法，支持广播\n",
    "* `torch.subtract`：等价于`torch.sub`\n",
    "\n",
    "### 乘法\n",
    "\n",
    "我们最常用的操作就是乘法，各类乘法也是PyTorch中最复杂、最令人困惑的操作。\n",
    "\n",
    "最常用的是`torch.bmm`和`torch.matmul`，都用来进行矩阵乘法。\n",
    "\n",
    "* `a * b`相当于`torch.mul(a, b)`\n",
    "* `a @ b`相当于`torch.matmul(a, b)`\n",
    "\n",
    "* `torch.mul`：逐点乘积，或者叫哈达玛积（Hadamard product），支持广播\n",
    "* `torch.multiply`：相当于`torch.mul`\n",
    "* `torch.mm`：2维tensor的矩阵乘法，不支持广播\n",
    "* `torch.bmm`：3维tensor的批量矩阵乘法，第一个维度是batch size，不支持广播\n",
    "* `torch.matmul`：通用的矩阵乘法，支持广播 *（最灵活，使用时检查参数维度，以免出bug）*\n",
    "    * 如果两个tensor都是1维的，进行点积（相当于`torch.dot`）\n",
    "        * 参数形状为`(n,)`和`(n,)`，结果形状为`()`\n",
    "    * 如果两个tensor都是2维的，进行矩阵乘法（相当于`torch.mm`）\n",
    "        * 参数形状为`(m, n)`和`(n, p)`，结果形状为`(m, p)`\n",
    "    * 如果第一个tensor是1维的，第二个tensor是二维的，等于行向量乘矩阵\n",
    "        * 参数形状为`(m,)`和`(m, n)`，结果形状为`(n,)`\n",
    "    * 如果第一个tensor是2维的，第二个tensor是一维的，等于矩阵乘列向量\n",
    "        * 参数形状为`(m, n)`和`(n,)`，结果形状为`(m,)`\n",
    "    * 如果两个tensor都至少1维，且至少有一个tensor至少是3维的，对batch维度进行广播后进行矩阵乘法\n",
    "        * 两个tensor的后两个维度必须满足矩阵乘法规则，然后对前面的维度进行广播\n",
    "        * 参数形状为`(*batch, m, n)`和`(*batch, n, p)`，结果形状为`(*batch, n, p)`\n",
    "* `torch.dot`：向量点积（dot product），或叫数量积（scalar product），两个tensor必须都是1维的且长度相同\n",
    "* `torch.outer`：向量外积（outer product），两个tensor必须都是1维的，结果是一个矩阵\n",
    "    * 参数形状为`(m,)`和`(n,)`，结果形状为`(m, n)`，结果的`torch.outer(a, b)[i, j] == a[i] * b[j]`\n",
    "* `torch.cross`：向量叉积（cross product），有时候也叫外积（exterior product），两个tensor必须相同形状且有一个维度是3，不支持广播\n",
    "    * 参数和结果形状都是`(*dim1, 3, *dim2)`\n",
    "    * 可以指定维度，如果不指定，会选择最开头的为3的维度，最好指定以避免bug\n",
    "\n",
    "### 除法\n",
    "\n",
    "* `a / b`相当于`torch.div(a, b)`\n",
    "* `a // b`相当于`torch.floor_divide(a, b)`\n",
    "* `torch.div`：浮点数除法，目前版本不支持两个整数tensor的除法，对整数tensor可以用`//`或者`torch.floor_divide`\n",
    "* `torch.true_divide`：等价于`torch.div`\n",
    "* `torch.floor_divide`：**向零舍入** ，如果两个参数都是整数，则结果为整数，否则为浮点数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather和scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
